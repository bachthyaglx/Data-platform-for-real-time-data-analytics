services:
  # =================== FLINK should be activated first, to listen and process Kafka ====================
  # ==================== FLINK ====================
  jobmanager:
    build: ./flink
    container_name: jobmanager
    ports:
      - "8081:8081" # Flink Web UI
    command: jobmanager
    volumes:
      - .:/data/
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
    depends_on:
      - hive-metastore
      - minio

  taskmanager:
    build: ./flink
    container_name: taskmanager
    command: taskmanager
    depends_on:
      - jobmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 4

  # ==================== KAFKA PRODUCER ====================
  kafka_producer:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka_producer
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped

  # ==================== FLINK JOB AUTO-SUBMIT ====================
  flink_job_submitter:
    build: ./flink
    container_name: flink_job_submitter
    depends_on:
      - jobmanager
      - taskmanager
      - kafka_producer
    command: >
      bash -c "
      echo '‚è≥ Waiting for Flink to be ready...' &&
      sleep 20 &&
      echo 'üöÄ Submitting Flink job...' &&
      /opt/flink/bin/sql-client.sh -f /opt/flink/jobs/kafka-to-iceberg.sql &&
      echo '‚úÖ Job submitted!'
      "
    restart: "no"

  # ==================== KAFKA ====================
  # ==================== Zookeeper to manage and distribute Kafka brokers ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.1
    container_name: zookeeper
    volumes:
      - ./data/zookeeper:/var/lib/zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.1
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    volumes:
      - ./data/kafka:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics",
          "--bootstrap-server",
          "localhost:9092",
          "--list",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==================== KAFKA UI ====================
  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000" # Kafdrop Web UI
    environment:
      KAFKA_BROKERCONNECT: broker:9092

  # ==================== ICEBERG needs 3 layers (catalog, metadata, data) ====================
  # ==================== ICEBERG: CATALOG layer (Hive Metastore / hms) ====================
  # Component 1Ô∏è‚É£: Hive Metastore (HMS)
  # - Feature: CATALOG - Manage metadata pointers
  # - L∆∞u tr·ªØ: Con tr·ªè t·ªõi current metadata version
  # - ACID: Atomic compare-and-swap cho transactions
  # - Database: Derby (embedded) ho·∫∑c PostgreSQL/MySQL
  # - Storage: ./data/hms/metastore_db/
  hive-metastore:
    build: ./hms-standalone-s3
    container_name: hms
    ports:
      - "9083:9083" # Thrift API
    volumes:
      - ./data/hms:/opt/hive/metastore
    environment:
      HMS_LOGLEVEL: INFO

  # ==================== ICEBERG: METADATA + DATA STORAGE layer (MinIO) ====================
  # Components 2Ô∏è‚É£ + 3Ô∏è‚É£: MinIO (S3-compatible Object Storage)
  # - Feature: Store both "metadata files" and "data" files
  # - 2Ô∏è‚É£ METADATA: JSON, Avro files
  #   Path: /data/warehouse/db/{table}/metadata/
  #   Files: v*.metadata.json, snap-*.avro, manifest-*.avro
  #   Size: ~1-5 MB per table
  # - 3Ô∏è‚É£ DATA: Parquet/ORC/Avro files
  #   Path: /data/warehouse/db/{table}/data/
  #   Files: Actual data records
  #   Size: GBs-TBs
  # - Storage: ./data/minio/warehouse/
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9001:9001" # MinIO Console UI
    volumes:
      - ./data/minio:/data
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
    command: ["server", "/data", "--console-address", ":9001"]

  # MinIO Client: Initialize warehouse bucket
  mc:
    image: minio/mc
    container_name: mc
    depends_on:
      - minio
    entrypoint: |
      /bin/sh -c "
      until mc config host add minio http://minio:9000 admin password; do 
        echo 'Waiting for MinIO...' && sleep 1; 
      done;
      mc mb -p minio/warehouse;
      tail -f /dev/null
      "

  # ==================== CLIENTS ====================
  pyiceberg:
    image: python:3.12-bookworm
    container_name: pyiceberg
    depends_on:
      - hive-metastore
      - minio
    environment:
      PYICEBERG_CATALOG__DEFAULT__URI: thrift://hms:9083
      PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID: admin
      PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY: password
      PYICEBERG_CATALOG__DEFAULT__S3__PATH_STYLE_ACCESS: "true"
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: http://minio:9000
    entrypoint: |
      /bin/sh -c "
      pip install -q pyiceberg[s3fs,hive,pyarrow];
      sleep infinity
      "

networks:
  default:
    name: zaphod
