services:
  # ==================== FLINK ====================
  jobmanager:
    build: ./flink
    hostname: jobmanager
    container_name: jobmanager
    depends_on:
      - minio
      - hive-metastore
    ports:
      - "8081:8081"
    command: ["bash", "/data/flink/submit_job.sh"]
    volumes:
      - .:/data/
      - flink-ready:/tmp
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        rest.flamegraph.enabled: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/jobs"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s

  taskmanager:
    build: ./flink
    hostname: taskmanager
    depends_on:
      - jobmanager
    command: taskmanager
    deploy:
      replicas: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 4

  # ==================== KAFKA PRODUCER ====================
  kafka_producer:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka_producer
    depends_on:
      kafka:
        condition: service_healthy
      jobmanager:
        condition: service_started # Wait for container to start
    volumes:
      - flink-ready:/tmp # Shared volume
    command: >
      /bin/bash -c "
      echo 'Waiting for Flink job to be ready...';
      until [ -f /tmp/flink-job-ready ]; do
        echo 'Waiting for Flink job...';
        sleep 1;
      done;
      echo '✓ Flink job is ready, starting Kafka producer...';
      python producer.py;
      "
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    container_name: broker
    image: confluentinc/cp-kafka:7.5.1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics",
          "--bootstrap-server",
          "localhost:9092",
          "--list",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000" # Kafdrop Web UI
    environment:
      KAFKA_BROKERCONNECT: broker:9092

  # ==================== ICEBERG needs 3 layers (catalog, metadata, data) ====================
  # ==================== ICEBERG: CATALOG layer (Hive Metastore / hms) ====================
  # Component 1️⃣: Hive Metastore (HMS)
  # - Feature: CATALOG - Manage metadata pointers
  # - Lưu trữ: Con trỏ tới current metadata version
  # - ACID: Atomic compare-and-swap cho transactions
  # - Database: Derby (embedded) hoặc PostgreSQL/MySQL
  # - Storage: ./data/hms/metastore_db/
  hive-metastore:
    build: ./iceberg/hms-standalone-s3
    container_name: hms
    depends_on:
      - minio
    ports:
      - "9083:9083"
    environment:
      HMS_LOGLEVEL: INFO
    healthcheck:
      test: ["CMD", "bash", "-c", "exec 6<> /dev/tcp/localhost/9083"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==================== ICEBERG: METADATA + DATA STORAGE layer (MinIO) ====================
  # Components 2️⃣ + 3️⃣: MinIO (S3-compatible Object Storage)
  # - Feature: Store both "metadata files" and "data" files
  # - 2️⃣ METADATA: JSON, Avro files
  #   Path: /data/warehouse/db/{table}/metadata/
  #   Files: v*.metadata.json, snap-*.avro, manifest-*.avro
  #   Size: ~1-5 MB per table
  # - 3️⃣ DATA: Parquet/ORC/Avro files
  #   Path: /data/warehouse/db/{table}/data/
  #   Files: Actual data records
  #   Size: GBs-TBs
  # - Storage: ./data/minio/warehouse/
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9091:9000"
      - "9001:9001" # MinIO UI
    command: ["server", "--console-address", ":9001", "/data"]
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password

  createbuckets:
    image: quay.io/minio/mc:latest
    container_name: createbuckets
    depends_on:
      - minio
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc alias set minio http://minio:9000 admin password;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      exit 0;
      "

  # ==================== CLIENTS ====================
  pyiceberg:
    image: python:3.12-bookworm
    container_name: pyiceberg
    depends_on:
      - hive-metastore
      - minio
    environment:
      PYICEBERG_CATALOG__DEFAULT__URI: thrift://hms:9083
      PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID: admin
      PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY: password
      PYICEBERG_CATALOG__DEFAULT__S3__PATH_STYLE_ACCESS: "true"
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: http://minio:9000
    entrypoint: |
      /bin/sh -c "
      pip install -q pyiceberg[s3fs,hive,pyarrow];
      sleep infinity
      "

volumes:
  flink-ready:

networks:
  default:
    name: zaphod
